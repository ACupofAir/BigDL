{"cells":[{"cell_type":"markdown","metadata":{},"source":[" [View the runnable example on GitHub](https://github.com/intel-analytics/BigDL/tree/main/python/nano/tutorial/notebook/inference/pytorch/accelerate_pytorch_inference_jit_ipex.ipynb)"]},{"cell_type":"markdown","metadata":{},"source":[" # Accelerate PyTorch Inference using jit&ipex"]},{"cell_type":"markdown","metadata":{},"source":[" > 📝 **Note**\n","> * `jit`: You can use `InferenceOptimizer.trace(..., accelerator=\"jit\")` API to enable the jit acceleration for PyTorch inference.\n","> * `ipex`: You can use`InferenceOptimizer.trace(...,use_ipex=True)` API to enable the ipex acceleration for PyTorch inference. It only takes a few lines.\n","> * `jit + ipex`: It is recommended to use JIT and IPEX together. You can user `InferenceOptimizer.trace(..., acclerator=\"jit\", use_ipex=True`) to enable both for PyTorch inference."]},{"cell_type":"markdown","metadata":{},"source":[" To apply JIT/IPEX acceleration, the following dependencies need to be installed first："]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for BigDL-Nano\n","!pip install --pre --upgrade bigdl-nano[pytorch] # install the nightly-bulit version\n","# !source bigdl-nano-init"]},{"cell_type":"markdown","metadata":{},"source":[" > 📝 **Note**\n"," >\n"," > We recommend to run the commands above, especially `source bigdl-nano-init` before jupyter kernel is started, or some of the optimizations may not take effect."]},{"cell_type":"markdown","metadata":{},"source":[" Let's take an [ResNet-18 model](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html) pretrained on ImageNet dataset as an example. First, we load the model:"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/cpx/anaconda3/envs/junwang-resnext-oob/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","/home/cpx/anaconda3/envs/junwang-resnext-oob/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/home/cpx/anaconda3/envs/junwang-resnext-oob/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}],"source":["import torch\n","from torchvision.models import resnet18\n","\n","model_ft = resnet18(pretrained=True)"]},{"cell_type":"markdown","metadata":{},"source":[" Then we set it in evaluation mode:"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",")"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["model_ft.eval()"]},{"cell_type":"markdown","metadata":{},"source":[" Normal Inference"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([111, 111])\n"]}],"source":["x = torch.rand(2, 3, 224, 224)\n","y_hat = model_ft(x)\n","predictions = y_hat.argmax(dim=1)\n","print(predictions)"]},{"cell_type":"markdown","metadata":{},"source":[" Accelerated Inference Using JIT/IPEX/JIT+IPEX, we need import `InferenceOptimizer` firstly."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from bigdl.nano.pytorch import InferenceOptimizer"]},{"cell_type":"markdown","metadata":{},"source":[" JIT Optimizer"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-11-17 13:37:17,867 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpdywe5gyl\n","2022-11-17 13:37:17,868 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpdywe5gyl/_remote_module_non_scriptable.py\n"]},{"name":"stdout","output_type":"stream","text":["tensor([111, 111])\n"]}],"source":["jit_model = InferenceOptimizer.trace(model_ft,\n","                                        accelerator=\"jit\",\n","                                        input_sample=torch.rand(1, 3, 224, 224))\n","y_hat = jit_model(x)\n","predictions = y_hat.argmax(dim=1)\n","print(predictions)"]},{"cell_type":"markdown","metadata":{},"source":[" IPEX Optimizer"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([111, 111])\n"]}],"source":["ipex_model = InferenceOptimizer.trace(model_ft,\n","                                        use_ipex=True)\n","y_hat = ipex_model(x)\n","predictions = y_hat.argmax(dim=1)\n","print(predictions)"]},{"cell_type":"markdown","metadata":{},"source":[" IPEX + JIT"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([111, 111])\n"]}],"source":["jit_ipex_model = InferenceOptimizer.trace(model_ft,\n","                                            accelerator=\"jit\",\n","                                            use_ipex=True,\n","                                            input_sample=torch.rand(1, 3, 224, 224))\n","y_hat = jit_ipex_model(x)\n","predictions = y_hat.argmax(dim=1)\n","print(predictions)"]},{"cell_type":"markdown","metadata":{},"source":[" > 📝 **Note**\n"," >\n"," > `input_sample` is the parameter for OpenVINO accelerator to know the **shape** of the model input. So both the batch size and the specific values are not important to `input_sample`. If we want our test dataset to consist of images with $224 \\times 224$ pixels, we could use `torch.rand(1, 3, 224, 224)` for `input_sample` here.\n"," >\n"," > Please refer to [API documentation](https://bigdl.readthedocs.io/en/latest/doc/PythonAPI/Nano/pytorch.html#bigdl.nano.pytorch.InferenceOptimizer.trace) for more information on `InferenceOptimizer.trace`."]},{"cell_type":"markdown","metadata":{},"source":[" > 📚 **Related Readings**\n"," >\n"," > - [How to install BigDL-Nano](https://bigdl.readthedocs.io/en/latest/doc/Nano/Overview/nano.html#install)\n"," > - [How to install BigDL-Nano in Google Colab](https://bigdl.readthedocs.io/en/latest/doc/Nano/Howto/install_in_colab.html)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.13 ('junwang-resnext-oob')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"2c2c667a59d63f4d9cf9e9a8f7eff73ad81424da777ad3c4a3346b0ce2b012b2"}}},"nbformat":4,"nbformat_minor":2}
